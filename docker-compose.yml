version: "3.9"

services:
  llm-service:
    image: ollama/ollama:latest          # always pin a tag you trust in prod
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"        # share the same GPU
      # NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # Optional: limit Ollamaâ€™s GPU memory footprint
      # OLLAMA_MAX_GPU_MEMORY: "20GiB"     # tweak for your card
    volumes:
      - ollama:/root/.ollama
    expose:
      - "11434"
    networks: [app-net]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 3 && ollama pull llama3 && wait"]

  image-generator:
    build: ./image-generator             # see Dockerfile tips below
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"
      # Optimize PyTorch memory for shared GPU environment
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:256,expandable_segments:True
      # Limit memory usage for SDXL models
      PYTORCH_CUDA_MEMORY_FRACTION: "0.3"  # Use max 30% of GPU memory (~10GB)
    expose:
      - "5001"
    networks: [app-net]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  video-generator:
    build: ./video-generator
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"        # Use RTX 5090
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # PyTorch memory optimization for shared GPU environment
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:256,expandable_segments:True
      # HuggingFace cache directory
      HF_HOME: /app/.cache/huggingface
      # RTX 5090 optimizations (CUDA 12.9 + driver 575.51.03)
      CUDA_VISIBLE_DEVICES: "0"
      TORCH_CUDA_ARCH_LIST: "8.9;9.0;12.0"   # RTX 5090 Blackwell (sm_120) + Hopper + Ada Lovelace
      # Limit memory for shared GPU (LLM: 8GB, Image: 10GB, Video: 12GB remaining)
      PYTORCH_CUDA_MEMORY_FRACTION: "0.4"  # Use max 40% of GPU memory (~13GB)
      CUDA_LAUNCH_BLOCKING: "0"  # Enable for performance
    volumes:
      # Cache AI models to avoid re-downloading
      - video_models:/app/.cache/huggingface
    networks: [app-net]
    expose:
      - "5003"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
        limits:
          # Conservative memory limit for shared environment
          memory: 16G

  poster-service:
    build: ./poster-service
    networks: [app-net]
    expose:
      - "5002"

  orchestrator:
    build: ./orchestrator
    depends_on:
      - llm-service
      - image-generator
      - video-generator
      - poster-service
    networks: [app-net]
    ports:
      - "8000:8000"

volumes:
  ollama:
  video_models:  # Cache for AI video models

networks:
  app-net:

