version: "3.9"

services:
  llm-service:
    image: ollama/ollama:latest          # always pin a tag you trust in prod
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"        # share the same GPU
      # NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # Optional: limit Ollamaâ€™s GPU memory footprint
      # OLLAMA_MAX_GPU_MEMORY: "20GiB"     # tweak for your card
    volumes:
      - ollama:/root/.ollama
    expose:
      - "11434"
    networks: [app-net]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 3 && ollama pull llama3 && wait"]

  image-generator:
    build: ./image-generator             # see Dockerfile tips below
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"
      # Optimize PyTorch memory for shared GPU environment
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:256,expandable_segments:True
      # Limit memory usage for SDXL models
      PYTORCH_CUDA_MEMORY_FRACTION: "0.3"  # Use max 30% of GPU memory (~10GB)
    expose:
      - "5001"
    networks: [app-net]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  video-generator:
    build: ./video-generator
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HOME: /app/.cache/huggingface
    volumes:
      - video_models:/app/.cache/huggingface
    networks: [app-net]
    expose:
      - "5003"
    restart: unless-stopped

  poster-service:
    build: ./poster-service
    networks: [app-net]
    expose:
      - "5002"

  orchestrator:
    build: ./orchestrator
    depends_on:
      - llm-service
      - image-generator
      - video-generator
      - poster-service
    networks: [app-net]
    ports:
      - "8000:8000"

volumes:
  ollama:
  video_models:  # Cache for AI video models

networks:
  app-net:

