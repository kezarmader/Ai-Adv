version: "3.9"

services:
  llm-service:
    image: ollama/ollama:latest          # always pin a tag you trust in prod
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"        # share the same GPU
      # NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # Optional: limit Ollamaâ€™s GPU memory footprint
      # OLLAMA_MAX_GPU_MEMORY: "20GiB"     # tweak for your card
    volumes:
      - ollama:/root/.ollama
    expose:
      - "11434"
    networks: [app-net]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 3 && ollama pull llama3 && wait"]

  image-generator:
    build: ./image-generator             # see Dockerfile tips below
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"
      # NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # Optional PyTorch memory tuner
      # PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:128
    expose:
      - "5001"
    networks: [app-net]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  video-generator:
    build: ./video-generator
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"        # Use RTX 5090
      # NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # PyTorch memory optimization for large models
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:512
      # HuggingFace cache directory
      HF_HOME: /app/.cache/huggingface
      # Additional GPU optimizations
      CUDA_VISIBLE_DEVICES: "0"
      TORCH_CUDA_ARCH_LIST: "8.9;9.0;12.0"   # RTX 5090 Blackwell (sm_120) + Hopper + Ada Lovelace
      # Enable debugging for RTX 5090 compatibility
      CUDA_LAUNCH_BLOCKING: "1"
    volumes:
      # Cache AI models to avoid re-downloading
      - video_models:/app/.cache/huggingface
    networks: [app-net]
    expose:
      - "5003"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
        limits:
          # Allow video generator to use more memory for large models
          memory: 16G

  poster-service:
    build: ./poster-service
    networks: [app-net]
    expose:
      - "5002"

  orchestrator:
    build: ./orchestrator
    depends_on:
      - llm-service
      - image-generator
      - video-generator
      - poster-service
    networks: [app-net]
    ports:
      - "8000:8000"

volumes:
  ollama:
  video_models:  # Cache for AI video models

networks:
  app-net:

